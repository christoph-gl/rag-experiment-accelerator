# Evaluation Metrics

The following is an overview of the available evaluation metrics that can be used to evaluate end-to-end performance of
a RAG application by measuring a distance between the ground truth answer and the predicted answer.

These metrics are calculated as part of  the `04_evaluation.py` script based on the `actual`, `expected` and `context` fields of
the `.jsonl` output file (referred to as "calculation base"), generated by `03_querying.py` script. See the [script inputs and outputs
guide](/docs/script-inputs-outputs.md#03_queryingpy) for more information.

You can choose which metrics should be calculated in your experiment by updating the `metric_types` field in the
`search_config.json` configuration file.

## Configuration Example

```json
"metric_types": [
    "lcsstr",
    "lcsseq",
    "cosine",
    "jaro_winkler",
    "hamming",
    "jaccard",
    "levenshtein",
    "fuzzy",
    "bert_all_MiniLM_L6_v2",
    "bert_base_nli_mean_tokens",
    "bert_large_nli_mean_tokens",
    "bert_large_nli_stsb_mean_tokens",
    "bert_distilbert_base_nli_stsb_mean_tokens",
    "bert_paraphrase_multilingual_MiniLM_L12_v2",
    "llm_answer_relevance",
    "llm_context_precision"
]
```

## Algorithm-based Metrics

The following metrics are calculated by using different string similarity algorithms mostly backed by the [TextDistance
Python package](https://pypi.org/project/textdistance/).

### Longest common substring

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `lcsstr`          | `actual`, `expected` | Percentage (0-100) |

Calculates the longest common substring (LCS) similarity score between two strings.

### Longest common subsequence

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `lcsseq`          | `actual`, `expected` | Percentage (0-100) |

Computes the longest common subsequence (LCS) similarity score between two input strings.

### Cosine similarity

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `cosine`          | `actual`, `expected` | Percentage (0-100) |

The cosine similarity is calculated as the cosine of the angle between two strings represented as vectors.

### Jaro-Winkler distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `jaro_winkler`    | `actual`, `expected` | Percentage (0-100) |

The Jaro-Winkler similarity score is a measure of similarity between two strings. The Jaro-Winkler similarity score is
calculated as the number of characters that are different between the two strings divided by the number of characters
that are the same between the two strings.

### Hamming distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `hamming`         | `actual`, `expected` | Percentage (0-100) |

The Hamming distance is a measure of similarity between two strings. The Hamming distance is calculated as the number of
characters that are different between the two strings.

### Jaccard similarity

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `jaccard`         | `actual`, `expected` | Percentage (0-100) |

The Jaccard similarity is calculated as the number of elements in the intersection of the two sets divided by the number
of elements in the union of the two sets.

### Levenshtein distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `levenshtein`     | `actual`, `expected` | Percentage (0-100) |

The Levenshtein distance is a measure of similarity between two strings. The Levenshtein distance is calculated as the
minimum number of insertions, deletions, or substitutions required to transform one string into the other.

### FuzzyWuzzy similarity

| Configuration Key | Calculation Base     | Possible Values       |
| ----------------- | -------------------- | --------------------- |
| `fuzzy`           | `actual`, `expected` | Integer (Fuzzy score) |

This metric is backed by the [FuzzyWuzzy Python package](https://pypi.org/project/fuzzywuzzy/).
Calculates the fuzzy score between two documents using the levenshtein distance.

## BERT-based semantic similarity

The following set of metrics calculates semantic similarity between two strings as percentage of differences based on
embeddings created by different BERT models. Backed by the [sentence-transformers Python
package](https://pypi.org/project/sentence-transformers/).

| Calculation Base     | Possible Values    |
| -------------------- | ------------------ |
| `actual`, `expected` | Percentage (0-100) |

| Configuration Key                          | BERT Model                                   |
| ------------------------------------------ | -------------------------------------------- |
| bert_all_MiniLM_L6_v2                      | MiniLM L6 v2 model                           |
| bert_base_nli_mean_tokens                  | Base model, mean tokens                      |
| bert_large_nli_mean_tokens                 | Large model, mean tokens                     |
| bert_large_nli_stsb_mean_tokens            | Large model, STS-B, mean tokens              |
| bert_distilbert_base_nli_stsb_mean_tokens  | DistilBERT base model, STS-B, mean tokens    |
| bert_paraphrase_multilingual_MiniLM_L12_v2 | Multilingual paraphrase model, MiniLM L12 v2 |

## LLM-based Metrics

The following metrics are calculated based on LLM reasoning. These metrics require the OpenAI endpoint to be configured
(see [Environment Variables](./environment-variables.md)).

These metrics also require the `chat_model_name` property to be set in the `search_config.json` configuration file. See
[Description of configuration elements](../README.md#description-of-configuration-elements) for details.

### LLM Answer relevance

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `llm_answer_relevance` | `actual`, `expected` | From 0 to 1 with 1 being the best |

Scores the relevancy of the answer according to the given question. Answers with incomplete, redundant or unnecessary
information is penalized.

### LLM Context precision

| Configuration Key   | Calculation Base    | Possible Values                                                   |
| ------------------- | ------------------- | ----------------------------------------------------------------- |
| `llm_context_precision` | `actual`, `context` | 1 (yes) or 0 (no) depending on if the context is relevant or not. |

Checks whether or not the context generated by the RAG solution is useful for answering a question.

## RAGAS Metrics

[RAGAS](https://docs.ragas.io/en/latest/index.html) is an open source framework offering a suite of metrics to evaluate both the end-to-end performance of RAG pipelines as well as the performance of each RAG component in isolation (retrieval, generation). 

To compute llm-based metrics, RAGAS requires the OpenAI endpoint to be configured (see [Environment Variables](./environment-variables.md)). These metrics also require the `azure_oai_eval_model_name` and `azure_oai_eval_deployment_name` as well as the `azure_oai_eval_embedding_model_name` and `azure_oai_eval_embedding_deployment_name` properties to be set in the configuration file. See
[Description of configuration elements](../README.md#description-of-configuration-elements) for details.

### Faithfulness

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_faithfulness` | `actual`, `retrieved_contexts` | From 0% to 100% with 100% being the best |

Measures the factual consistency of the generated answer against the given context. Answers with claims not inferred from the retrieved contexts are penalized. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html).

### Answer relevancy

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_answer_relevancy` | `question`, `actual` | From 0% to 100% with 100% being the best |

Assesses how pertinent the generated answer is to the given prompt or user query. Answers with incomplete, redundant or unnecessary
information are penalized. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html).

### Context precision

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_context_precision` | `question`, `expected`, `retrieved_contexts` | From 0% to 100% with 100% being the best |

Evaluates whether all the ground-truth relevant items present in the retrieved contexts are ranked higher or not. Ideally, all the relevant chunks must appear at the top ranks. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html).

### Context relevancy

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_context_relevancy` | `question`, `retrieved_contexts` | From 0% to 100% with 100% being the best |

Gauges the relevancy of the retrieved context to the query. Penalizes retrieved contexts containing irrelevant information to answer the question. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html).

### Context recall

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_context_recall` | `expected`, `retrieved_contexts` | From 0% to 100% with 100% being the best |

Measures the extent to which the retrieved context aligns with the ground truth answer. In an ideal scenario, all sentences in the ground truth answer are attributable to the retrieved context. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html).

### Context entity recall

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_context_entity_recall` | `expected`, `retrieved_contexts` | From 0% to 100% with 100% being the best |

Measures the recall of the retrieved context based on the number of entities present in both the ground truth answer and the retrieved context relative to the number of entities present in the ground truth. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/context_entities_recall.html).

### Answer correctness

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_answer_correctness` | `actual`, `expected` | From 0% to 100% with 100% being the best |

Measures the accuracy of the generated answer when compared to the ground truth. It encompasses two aspects combined using a weighted scheme: the semantic similarity between the generated answer and the ground truth, as well as the factual similarity. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html).

### Answer semantic similarity

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `ragas_answer_similarity` | `actual`, `expected` | From 0% to 100% with 100% being the best |

Scores the semantic resemblance between the generated answer and the ground truth. More info [here](https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html).
